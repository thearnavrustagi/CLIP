# CLIP model for image captioning

Uses a CLIP based model using cross attention, and causal attention layers, with embedding and an output layer to caption images, works best with temperature set to 0.5<br>

there are still some formatting errors, in some cases making faulty datasets, also remakes dataset shards if run again<br><br>
Much to contrary practice running `model.py` runs the program, `main.py` doesn't even function
